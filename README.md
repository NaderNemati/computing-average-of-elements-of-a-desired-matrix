# Computing Average of Elements in a Target Matrix

Welcome to the repository dedicated to the task of computing the average of elements within a specific matrix. This project was undertaken as part of my coursework in the domain of parallel algorithms. The challenge revolved around devising efficient algorithms that leverage parallel computing techniques to calculate the average of matrix elements.

## Exploring the Matrix Computation Domain

Matrix operations form a fundamental cornerstone of computational mathematics. In this endeavor, I embarked on a journey to tackle the task of computing the average of matrix elements, a seemingly simple yet crucial operation with numerous real-world applications.

## Diverse Platform Utilization

To tackle this matrix computation challenge, I ventured into the realm of parallel programming, employing three distinct platforms: CUDA, MPI, and OpenMP. Each platform brings its unique capabilities to the table, enabling optimized execution and enhanced performance.

### CUDA Power Unleashed

With CUDA, I harnessed the immense power of GPUs to expedite the matrix computation process. I explored two distinct global function approaches: multi-block threads and single-block threads. By strategically distributing the computational workload across threads, I aimed to achieve efficient and rapid matrix element averaging.

### Exploring MPI on Ubuntu 18.04

The MPI version of the matrix computation was executed on an Ubuntu 18.04 environment. This approach enabled distributed computing, where multiple processes collaborate to accomplish the task. The utilization of MPI showcases the potential for parallelization across different computing units, contributing to optimized execution.

## Unveiling Results and Insights

Throughout this project, I sought to not only obtain accurate averages of matrix elements but also to gain insights into the efficiency and performance trade-offs inherent to different parallel computing paradigms. By comparing the outcomes of CUDA, MPI, and OpenMP implementations, I aimed to highlight the strengths and weaknesses of each approach.

## Collaboration and Future Explorations

The realm of parallel algorithms and matrix computations is a dynamic and evolving domain. I invite you to delve into the code, explore the intricacies of parallel programming across CUDA, MPI, and OpenMP, and engage in discussions about optimization, performance, and potential enhancements.

Your active involvement can contribute to the advancement of parallel computing methodologies and shed light on innovative ways to tackle matrix operations efficiently and effectively.

For inquiries, collaborations, or discussions, feel free to reach out to [Nader NEmati](mailto:nnevar@utu.fi).

